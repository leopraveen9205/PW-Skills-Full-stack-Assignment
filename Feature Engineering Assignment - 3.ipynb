{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "Min-Max scaling is a normalization technique used in data preprocessing to rescale the features of a dataset to a fixed range, typically [0, 1].\n",
    "This is achieved by transforming the original data using the formula:\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "where X is the original value, X_min is the minimum value of the feature, and X_max is the maximum value of the feature. \n",
    "This scaling method is particularly useful when the data does not follow a Gaussian distribution or when the features have different units or scales.  \n",
    "For example, consider a dataset with a feature representing age (ranging from 0 to 100) and another feature representing income (ranging from 0 to 1,000,000). \n",
    "Without scaling, the income feature would dominate the distance calculations in algorithms like k-nearest neighbors or clustering. \n",
    "By applying Min-Max scaling, both features can be rescaled to the range [0, 1], allowing them to contribute equally to the analysis.  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d936f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "The Unit Vector technique, also known as normalization, is a feature scaling method that transforms the data such that each data point has a unit norm (length of 1). This is achieved by dividing each feature vector by its Euclidean norm (L2 norm). The formula for unit vector normalization is:\n",
    "X_normalized = X / ||X||    \n",
    "where ||X|| is the Euclidean norm of the vector X.\n",
    "The key difference between Unit Vector scaling and Min-Max scaling is that Min-Max scaling rescales the features to a specific range (e.g., [0, 1]), while Unit Vector scaling focuses on the direction of the data points in the feature space, ensuring that each data point lies on the unit sphere.\n",
    "For example, consider a dataset with two features: height and weight. A data point with height 180 cm and weight 75 kg would have a Euclidean norm calculated as:\n",
    "||X|| = sqrt(180^2 + 75^2) = sqrt(32400 + 5625) = sqrt(38025) ≈ 194.96\n",
    "The normalized values would be:     \n",
    "Height_normalized = 180 / 194.96 ≈ 0.923\n",
    "Weight_normalized = 75 / 194.96 ≈ 0.385 \n",
    "Thus, the normalized data point would be approximately (0.923, 0.385), which lies on the unit sphere.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction while preserving as much variance as possible in the dataset. It transforms the original features into a new set of uncorrelated variables called principal components, which are ordered by the amount of variance they capture from the data.\n",
    "The steps involved in PCA include:\n",
    "1. Standardizing the data to have a mean of 0 and a standard deviation of 1.\n",
    "2. Computing the covariance matrix to understand how the features vary with respect to each other.  \n",
    "3. Calculating the eigenvalues and eigenvectors of the covariance matrix to identify the principal components.\n",
    "4. Selecting the top k principal components that capture the most variance.\n",
    "5. Projecting the original data onto the new subspace formed by the selected principal components.  \n",
    "For example, consider a dataset with three features: height, weight, and age. By applying PCA, we can reduce the dimensionality from three to two dimensions while retaining most of the variance in the data. \n",
    "After performing PCA, we might find that the first two principal components capture 95% of the variance.\n",
    "We can then project the original data points onto these two principal components, effectively reducing the dataset to two dimensions while still preserving the essential information.      \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76058e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "PCA (Principal Component Analysis) is a widely used technique for feature extraction, which involves transforming the original features into a new set of features that capture the most important information in the data. The relationship between PCA and feature extraction lies in PCA's ability to identify the directions (principal components) in which the data varies the most, allowing us to extract these components as new features.\n",
    "PCA can be used for feature extraction by following these steps:\n",
    "1. Standardizing the data to ensure that each feature contributes equally to the analysis.\n",
    "2. Computing the covariance matrix to understand the relationships between features.\n",
    "3. Calculating the eigenvalues and eigenvectors of the covariance matrix to identify the principal components.\n",
    "4. Selecting the top k principal components based on their eigenvalues, which represent the amount of variance captured by each component.\n",
    "5. Projecting the original data onto the selected principal components to create a new feature set.\n",
    "For example, consider a dataset with four features: height, weight, age, and income. By applying PCA, we can identify that the first two principal components capture 90% of the variance in the data.\n",
    "We can then extract these two principal components as new features, effectively reducing the dimensionality of the dataset from four to two while retaining most of the important information.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "To preprocess the dataset for the recommendation system using Min-Max scaling, I would follow these steps:\n",
    "1. Identify the features to be scaled: In this case, the features are price, rating, and delivery time.\n",
    "2. Calculate the minimum and maximum values for each feature:\n",
    "    - For price, find the minimum and maximum prices in the dataset.\n",
    "    - For rating, find the minimum and maximum ratings (e.g., 1 to 5).\n",
    "    - For delivery time, find the minimum and maximum delivery times in minutes.\n",
    "3. Apply the Min-Max scaling formula to each feature:\n",
    "    X_scaled = (X - X_min) / (X_max - X_min)\n",
    "    - For each price value, subtract the minimum price and divide by the range (max price - min price).\n",
    "    - For each rating value, subtract the minimum rating and divide by the range (max rating - min rating).\n",
    "    - For each delivery time value, subtract the minimum delivery time and divide by the range (max delivery time - min delivery time).\n",
    "4. Replace the original feature values with the scaled values in the dataset.\n",
    "For example, if the price ranges from $5 to $50, a price of $20 would be scaled as follows:\n",
    "Price_scaled = (20 - 5) / (50 - 5) = 15 / 45 ≈ 0.333\n",
    "Similarly, if the rating ranges from 1 to 5, a rating of 4 would be scaled as:\n",
    "Rating_scaled = (4 - 1) / (5 - 1) = 3 / 4 = 0.75\n",
    "And if the delivery time ranges from 10 to 60 minutes, a delivery time of 30 minutes would be scaled as:\n",
    "DeliveryTime_scaled = (30 - 10) / (60 - 10) = 20 / 50 = 0.4\n",
    "By applying Min-Max scaling, all features will be rescaled to the range [0, 1], ensuring that they contribute equally to the recommendation system's algorithms.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "To reduce the dimensionality of the dataset for predicting stock prices using PCA, I would follow these steps:\n",
    "1. Standardize the data: Since PCA is sensitive to the scale of the features, I would first standardize the dataset to have a mean of 0 and a standard deviation of 1 for each feature.\n",
    "2. Compute the covariance matrix: I would calculate the covariance matrix of the standardized data to understand how the features vary with respect to each other.\n",
    "3. Calculate eigenvalues and eigenvectors: I would compute the eigenvalues and eigenvectors of the covariance matrix to identify the principal components.\n",
    "4. Select principal components: I would sort the eigenvalues in descending order and select the top k principal components that capture a significant amount of variance (e.g., 95% of the total variance).\n",
    "5. Project the data: I would project the original standardized data onto the selected principal components to create a new dataset with reduced dimensionality.\n",
    "For example, if the original dataset has 50 features related to company financial data and market trends, after applying PCA, I might find that the first 10 principal components capture 95% of the variance in the data.\n",
    "I would then use these 10 principal components as the new features for building the stock price prediction model, effectively reducing the dimensionality from 50 to 10 while retaining most of the important information.      \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "To perform Min-Max scaling to transform the values [1, 5, 10, 15, 20] to a range of -1 to 1, we can use the following formula:\n",
    "X_scaled = (X - X_min) / (X_max - X_min) * (new_max - new_min) + new_min\n",
    "where new_min = -1 and new_max = 1.\n",
    "First, we need to identify the minimum and maximum values in the dataset:\n",
    "X_min = 1\n",
    "X_max = 20\n",
    "Next, we can apply the Min-Max scaling formula to each value in the dataset:    \n",
    "1. For X = 1:\n",
    "   X_scaled = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0 * 2 - 1 = -1\n",
    "2. For X = 5:   \n",
    "    X_scaled = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) = (4 / 19) * 2 - 1 ≈ -0.5789\n",
    "3. For X = 10:\n",
    "    X_scaled = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = (9 / 19) * 2 - 1 ≈ -0.0526\n",
    "4. For X = 15:\n",
    "    X_scaled = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) = (14 / 19) * 2 - 1 ≈ 0.4737\n",
    "5. For X = 20:\n",
    "    X_scaled = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 2 - 1 = 1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "To perform Feature Extraction using PCA on the dataset with features [height, weight, age, gender, blood pressure], I would follow these steps:\n",
    "1. Standardize the data: Since PCA is sensitive to the scale of the features, I would first standardize the dataset to have a mean of 0 and a standard deviation of 1 for each feature.\n",
    "2. Compute the covariance matrix: I would calculate the covariance matrix of the standardized data to understand how the features vary with respect to each other.\n",
    "3. Calculate eigenvalues and eigenvectors: I would compute the eigenvalues and eigenvectors of the covariance matrix to identify the principal components.\n",
    "4. Select principal components: I would sort the eigenvalues in descending order and select the top k principal components that capture a significant amount of variance (e.g., 95% of the total variance).\n",
    "5. Project the data: I would project the original standardized data onto the selected principal components to create a new dataset with reduced dimensionality.\n",
    "The number of principal components to retain would depend on the explained variance ratio. I would typically look for the \"elbow point\" in a scree plot, which shows the cumulative explained variance as a function of the number of principal components. If, for example, the first three principal components capture 95% of the variance, I would choose to retain those three components. This approach ensures that we reduce the dimensionality of the dataset while\n",
    "preserving most of the important information for further analysis or modeling.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931ebb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f608d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f75b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
