{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "1. What are missing values in a dataset?\n",
    "\n",
    "Missing values occur when no data value is stored for a particular variable (column) in an observation (row).\n",
    "They are usually represented as NaN, NULL, or blank cells.\n",
    "2. Why is it essential to handle missing values?\n",
    "\n",
    "Handling missing values is important because:\n",
    "\n",
    "Algorithms may fail\n",
    "\n",
    "Many ML algorithms cannot work with missing values.\n",
    "\n",
    "Incorrect results\n",
    "\n",
    "Missing data can bias predictions and reduce accuracy.\n",
    "\n",
    "Loss of valuable information\n",
    "\n",
    "Ignoring missing values may discard useful data.\n",
    "\n",
    "Better model performance\n",
    "\n",
    "Proper handling improves reliability and robustness.\n",
    "\n",
    "Data consistency\n",
    "\n",
    "Clean data ensures correct statistical analysis.\n",
    "\n",
    "3. Algorithms that are not affected by missing values\n",
    "\n",
    "Some algorithms can handle missing values internally or are less sensitive to them:\n",
    "\n",
    "Decision Trees\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Gradient Boosting (XGBoost, LightGBM)\n",
    "\n",
    "Naïve Bayes (in some implementations)\n",
    "\n",
    "k-Nearest Neighbors (KNN) (can handle with distance-based approaches)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b783fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "1. Removing rows with missing values\n",
    "\n",
    "Description:\n",
    "Rows containing missing values are deleted.\n",
    "\n",
    "When to use:\n",
    "\n",
    "When missing values are very few\n",
    "\n",
    "When dataset is large\n",
    "\n",
    "2. Removing columns with missing values\n",
    "\n",
    "Description:\n",
    "Columns with missing values are removed.\n",
    "\n",
    "When to use:\n",
    "\n",
    "When a column has too many missing values\n",
    "\n",
    "When column is not important\n",
    "\n",
    "3. Mean Imputation (Numerical data)\n",
    "\n",
    "Description:\n",
    "Missing values are replaced with the mean of the column.\n",
    "\n",
    "4. Median Imputation (Numerical data)\n",
    "\n",
    "Description:\n",
    "Missing values are replaced with the median of the column.\n",
    "Useful when data has outliers.\n",
    "\n",
    "5. Mode Imputation (Categorical data)\n",
    "\n",
    "Description:\n",
    "Missing values are replaced with the most frequent value.\n",
    "\n",
    "6. Forward Fill (Propagation method)\n",
    "\n",
    "Description:\n",
    "Missing value is replaced with the previous value.\n",
    "\n",
    "7. Backward Fill\n",
    "\n",
    "Description:\n",
    "Missing value is replaced with the next value.\n",
    "\n",
    "8. Using a Constant Value\n",
    "\n",
    "Description:\n",
    "Missing values are replaced with a fixed value like 0 or \"Unknown\".\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Imbalanced data occurs when the classes in a dataset are not equally represented, and one class has significantly more samples than the other(s).\n",
    "It is very common in classification problems.\n",
    "\n",
    "Most machine learning algorithms assume that all classes are equally important.\n",
    "When data is imbalanced, the model becomes biased toward the majority class.\n",
    "\n",
    "What happens if imbalanced data is not handled?\n",
    "1. High accuracy but poor performance\n",
    "\n",
    "Model may predict only the majority class.\n",
    "\n",
    "Example: Predicting Not Fraud for all cases gives 99% accuracy, but detects no fraud.\n",
    "\n",
    "2. Poor minority class prediction\n",
    "\n",
    "The model fails to identify rare but important cases (fraud, disease, defects).\n",
    "\n",
    "3. Misleading evaluation metrics\n",
    "\n",
    "Accuracy becomes meaningless.\n",
    "\n",
    "Metrics like precision, recall, F1-score are affected.\n",
    "\n",
    "4. Increased business risk  \n",
    "\n",
    "Missed fraud cases → financial loss\n",
    "\n",
    "Missed disease detection → health risk\n",
    "\n",
    "5. Overfitting to majority class\n",
    "\n",
    "Model learns patterns only from the dominant class.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down - sampling are required.\n",
    "\n",
    "Up-sampling is a technique used to handle imbalanced data by increasing the number of samples in the minority class so that it becomes comparable to the majority class.\n",
    "\n",
    "This is done by:\n",
    "\n",
    "Duplicating existing minority samples, or Generating synthetic samples (e.g., SMOTE).\n",
    "\n",
    "When up-sampling is required:\n",
    "\n",
    "When the dataset is small\n",
    "\n",
    "When removing majority data is risky\n",
    "\n",
    "When minority class is more important (fraud, disease detection)\n",
    "\n",
    "2. Down-sampling\n",
    "Definition:\n",
    "\n",
    "Down-sampling is a technique where the number of samples in the majority class is reduced to balance the dataset.\n",
    "\n",
    "This is done by randomly removing samples from the majority class.\n",
    "\n",
    "When down-sampling is required:\n",
    "\n",
    "When dataset is very large\n",
    "\n",
    "When computation cost is high\n",
    "\n",
    "When majority class has redundant data\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f621509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is Data Augmentation? Explain SMOTE\n",
    "\n",
    "Data augmentation is a technique used to increase the size and diversity of a dataset by creating new data samples from existing data without collecting new data.\n",
    "\n",
    "It is mainly used to:\n",
    "\n",
    "Handle imbalanced datasets\n",
    "\n",
    "Reduce overfitting\n",
    "\n",
    "Improve model generalization\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique used for imbalanced classification problems.\n",
    "\n",
    "It increases the minority class samples by creating synthetic (new) data points, not by duplicating existing ones.\n",
    "\n",
    "Select a minority class data point\n",
    "\n",
    "Find its k nearest minority neighbors\n",
    "\n",
    "Create a new synthetic point between them\n",
    "\n",
    "Repeat until classes are balanced\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Outliers are data points that significantly differ from the majority of the data values.\n",
    "They may occur due to measurement errors, data entry mistakes, or genuine rare events.\n",
    "\n",
    "2. Why is it essential to handle outliers?\n",
    "1. Prevents misleading results\n",
    "\n",
    "Outliers can distort mean, variance, and correlations.\n",
    "\n",
    "2. Improves model accuracy\n",
    "\n",
    "Many algorithms (Linear Regression, KNN, SVM) are sensitive to outliers.\n",
    "\n",
    "3. Better data visualization & analysis\n",
    "\n",
    "Outliers can hide true data patterns.\n",
    "\n",
    "4. Reduces model bias\n",
    "\n",
    "Model may overfit extreme values.\n",
    "\n",
    "5. Detects errors and anomalies\n",
    "\n",
    "Helps identify data entry or system errors.\n",
    "\n",
    "3. What happens if outliers are not handled?\n",
    "\n",
    "Poor model performance\n",
    "\n",
    "Wrong predictions\n",
    "\n",
    "Unstable and unreliable models\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f671bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "When working on customer data, missing values are common (e.g., missing age, income, location).\n",
    "Several techniques can be used to handle missing data effectively, depending on the situation.\n",
    "\n",
    "1. Remove Missing Data\n",
    "a) Deleting rows\n",
    "\n",
    "Remove records with missing values.\n",
    "\n",
    "Suitable when missing data is very small and random.\n",
    "\n",
    "Example:\n",
    "Remove customers with missing age. \n",
    "b) Deleting columns\n",
    "\n",
    "Remove entire columns if they have too many missing values.\n",
    "\n",
    "Useful when the column is not important.\n",
    "\n",
    "2. Statistical Imputation\n",
    "a) Mean Imputation\n",
    "\n",
    "Replace missing values with the average.\n",
    "\n",
    "Used for numerical features like income or spending.\n",
    "\n",
    "b) Median Imputation\n",
    "\n",
    "Replace with middle value.\n",
    "\n",
    "Best when data has outliers.\n",
    "\n",
    "c) Mode Imputation\n",
    "\n",
    "Replace with most frequent value.\n",
    "\n",
    "Used for categorical features like city or gender.\n",
    "\n",
    "3. Forward Fill and Backward Fill\n",
    "\n",
    "Forward fill: Use previous customers value.\n",
    "\n",
    "Backward fill: Use next available value.\n",
    "\n",
    "Mostly used in time-series customer data.\n",
    "\n",
    "4. Constant Value Replacement\n",
    "\n",
    "Replace missing values with a placeholder like \"Unknown\" or 0.\n",
    "\n",
    "Useful for categorical customer attributes.\n",
    "\n",
    "5. Model-Based Imputation\n",
    "\n",
    "Predict missing values using machine learning models.\n",
    "\n",
    "Example: Predict missing income based on age and occupation.\n",
    "\n",
    "6. Using Algorithms That Handle Missing Data\n",
    "\n",
    "Some algorithms handle missing values internally:\n",
    "\n",
    "Decision Trees\n",
    "\n",
    "Random Forest\n",
    "\n",
    "XGBoost\n",
    "\n",
    "7. Business-Aware Handling\n",
    "\n",
    "For critical fields (email, phone): record may be discarded\n",
    "\n",
    "For optional fields (age, income): imputation is preferred\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d803e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "1. Types of Missing Data (Conceptual)\n",
    "\n",
    "MCAR (Missing Completely At Random):\n",
    "Missingness has no relationship with any variable.\n",
    "\n",
    "MAR (Missing At Random):\n",
    "Missingness depends on other observed variables.\n",
    "\n",
    "MNAR (Missing Not At Random):\n",
    "Missingness depends on the missing value itself.\n",
    "\n",
    "2. Strategies to Identify Missing Data Patterns\n",
    "1. Calculate Missing Value Percentages\n",
    "\n",
    "Check how much data is missing in each column.\n",
    "\n",
    "Very small and uniform missingness often suggests randomness.\n",
    "\n",
    "Example:\n",
    "If 1 to 2% missing evenly across columns → likely random.\n",
    "\n",
    "2. Visual Inspection\n",
    "\n",
    "Use heatmaps or matrix plots to visualize missing values.\n",
    "\n",
    "Random missing data appears scattered.\n",
    "\n",
    "Pattern-based missing data appears clustered.\n",
    "\n",
    "Example:\n",
    "Income missing only for senior citizens → pattern exists.\n",
    "\n",
    "3. Compare Distributions\n",
    "\n",
    "Compare rows with missing values vs without missing values.\n",
    "\n",
    "If their distributions differ significantly, missingness is not random.\n",
    "\n",
    "Example:\n",
    "Customers with missing income mostly belong to a specific region.\n",
    "\n",
    "4. Correlation with Other Features\n",
    "\n",
    "Create a missing indicator column (1 = missing, 0 = not missing).\n",
    "\n",
    "Check correlation with other variables.\n",
    "\n",
    "Example:\n",
    "Income missing strongly correlated with employment type.\n",
    "\n",
    "5. Group-wise Analysis\n",
    "\n",
    "Analyze missing data by categories such as:\n",
    "\n",
    "Age group\n",
    "\n",
    "Gender\n",
    "\n",
    "Region\n",
    "\n",
    "Example:\n",
    "Phone numbers missing mostly for older customers.\n",
    "\n",
    "6. Statistical Tests\n",
    "\n",
    "Use tests like Littles MCAR test to formally test randomness.\n",
    "\n",
    "Helps decide whether data is MCAR or not.\n",
    "\n",
    "7. Business & Data Collection Understanding\n",
    "\n",
    "Understand how the data was collected.\n",
    "\n",
    "Survey skip logic or system failures often create patterns.\n",
    "\n",
    "Example:\n",
    "Optional survey questions → intentional missing data.\n",
    "\n",
    "3. Why this analysis is important\n",
    "\n",
    "Determines correct imputation strategy\n",
    "\n",
    "Avoids biased analysis\n",
    "\n",
    "Improves model accuracy and trustworthiness\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be58792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Evaluating Model Performance on an Imbalanced Medical Dataset\n",
    "\n",
    "In medical diagnosis datasets, it is common that most patients do not have the disease, while only a small percentage do.\n",
    "In such imbalanced datasets, using accuracy alone is misleading. Proper evaluation strategies are essential.\n",
    "\n",
    "1. Use Appropriate Evaluation Metrics (Instead of Accuracy)\n",
    "a) Confusion Matrix\n",
    "\n",
    "Shows TP, FP, TN, FN\n",
    "\n",
    "Helps understand how many sick patients are correctly or incorrectly classified\n",
    "\n",
    "Why important:\n",
    "False Negatives (missing a disease) are critical in healthcare.\n",
    "\n",
    "b) Precision\n",
    "\n",
    "Measures how many predicted positive cases are actually positive\n",
    "\n",
    "Important when false positives are costly\n",
    "\n",
    "c) Recall (Sensitivity)\n",
    "\n",
    "Measures how many actual disease cases are correctly detected\n",
    "\n",
    "Most important in medical diagnosis\n",
    "\n",
    "d) F1-Score\n",
    "\n",
    "Harmonic mean of precision and recall\n",
    "\n",
    "Useful when classes are imbalanced\n",
    "\n",
    "2. ROC Curve and AUC Score\n",
    "\n",
    "ROC Curve: Trade-off between True Positive Rate and False Positive Rate\n",
    "\n",
    "AUC: Overall model performance independent of threshold\n",
    "\n",
    "Why useful:\n",
    "Works well for imbalanced data comparison.\n",
    "\n",
    "3. Precision Recall Curve\n",
    "\n",
    "More informative than ROC when the positive class is rare\n",
    "\n",
    "Focuses on performance of the minority (disease) class\n",
    "\n",
    "4. Stratified Train-Test Split\n",
    "\n",
    "Ensures both train and test sets maintain the same class distribution\n",
    "\n",
    "Prevents biased evaluation\n",
    "\n",
    "5. Cost-Sensitive Evaluation\n",
    "\n",
    "Assign higher cost to False Negatives\n",
    "\n",
    "Evaluate whether the model minimizes critical medical errors\n",
    "\n",
    "6. Cross-Validation with Stratification\n",
    "\n",
    "Use Stratified K-Fold Cross Validation\n",
    "\n",
    "Ensures stable and reliable performance estimates\n",
    "\n",
    "7. Baseline Comparison\n",
    "\n",
    "Compare model against a naive baseline (e.g., always predicting “no disease”)\n",
    "\n",
    "Ensures model adds real value\n",
    "\n",
    "8. Domain-Specific Evaluation\n",
    "\n",
    "Collaborate with medical experts\n",
    "\n",
    "Evaluate whether predictions are clinically acceptable\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "Handling an Imbalanced Customer Satisfaction Dataset\n",
    "\n",
    "When most customers report being satisfied and only a few are unsatisfied, the dataset becomes imbalanced.\n",
    "To build a reliable model, the data must be balanced and the majority class may need to be down-sampled.\n",
    "\n",
    "1. Random Down-sampling of the Majority Class\n",
    "\n",
    "What it does:\n",
    "\n",
    "Randomly removes samples from the satisfied (majority) class.\n",
    "\n",
    "When to use:\n",
    "\n",
    "Dataset is large\n",
    "\n",
    "Majority class has redundant data\n",
    "\n",
    "Example:\n",
    "Reduce 90,000 satisfied customers to 10,000 to match unsatisfied customers.\n",
    "\n",
    "2. Stratified Sampling\n",
    "\n",
    "What it does:\n",
    "\n",
    "Maintains class proportions while sampling.\n",
    "\n",
    "Often combined with down-sampling.\n",
    "\n",
    "Use case:\n",
    "\n",
    "Creating balanced training data while keeping test data intact.\n",
    "\n",
    "3. Cluster-Based Down-sampling\n",
    "\n",
    "What it does:\n",
    "\n",
    "Groups majority class data into clusters\n",
    "\n",
    "Samples equally from each cluster to preserve diversity\n",
    "\n",
    "Benefit:\n",
    "\n",
    "Reduces information loss compared to random down-sampling.\n",
    "\n",
    "4. NearMiss Algorithm\n",
    "\n",
    "What it does:\n",
    "\n",
    "Selects majority class samples closest to minority class samples.\n",
    "\n",
    "Focuses learning on decision boundaries.\n",
    "\n",
    "Best for:\n",
    "\n",
    "Improving classification near class overlap regions.\n",
    "\n",
    "5. Tomek Links (Cleaning Majority Class)\n",
    "\n",
    "What it does:\n",
    "\n",
    "Removes overlapping majority samples near minority class.\n",
    "\n",
    "Helps clean noisy boundaries.\n",
    "\n",
    "6. Ensemble Methods with Balanced Sampling\n",
    "\n",
    "Examples:\n",
    "\n",
    "Balanced Random Forest\n",
    "\n",
    "EasyEnsemble\n",
    "\n",
    "Benefit:\n",
    "\n",
    "Each model is trained on a different down-sampled subset.\n",
    "\n",
    "7. Combine Down-sampling with Up-sampling (Hybrid)\n",
    "\n",
    "Example:\n",
    "\n",
    "Down-sample satisfied customers slightly\n",
    "\n",
    "Up-sample unsatisfied customers using SMOTE\n",
    "\n",
    "8. Keep Test Set Imbalanced\n",
    "\n",
    "Important note:\n",
    "\n",
    "Balance only the training data\n",
    "\n",
    "Keep test data reflecting real-world distribution\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "Handling Rare Events by Up-sampling the Minority Class\n",
    "\n",
    "When estimating the occurrence of a rare event (fraud, failure, disease), datasets are usually highly imbalanced, with very few positive cases.\n",
    "To improve model learning, the minority class must be up-sampled.\n",
    "\n",
    "1. Random Over-sampling\n",
    "\n",
    "What it does:\n",
    "\n",
    "Randomly duplicates existing minority class samples.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple to implement\n",
    "\n",
    "No data loss\n",
    "\n",
    "Limitation:\n",
    "\n",
    "Risk of overfitting due to duplication\n",
    "\n",
    "2. SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "What it does:\n",
    "\n",
    "Generates new synthetic minority samples using nearest neighbors.\n",
    "\n",
    "Why effective:\n",
    "\n",
    "Avoids exact duplication\n",
    "\n",
    "Improves generalization\n",
    "\n",
    "3. SMOTE Variants\n",
    "a) Borderline-SMOTE\n",
    "\n",
    "Generates samples near class boundaries\n",
    "\n",
    "Improves decision boundary learning\n",
    "\n",
    "b) ADASYN\n",
    "\n",
    "Creates more synthetic samples for harder-to-learn minority points\n",
    "\n",
    "4. Data Augmentation\n",
    "\n",
    "What it does:\n",
    "\n",
    "Creates new samples by modifying existing data\n",
    "\n",
    "Common in image, text, and signal data\n",
    "\n",
    "Example:\n",
    "\n",
    "Slightly altering sensor readings to simulate failures\n",
    "\n",
    "5. Ensemble Methods with Up-sampling\n",
    "\n",
    "Examples:\n",
    "\n",
    "EasyEnsemble\n",
    "\n",
    "Balanced Bagging\n",
    "\n",
    "Benefit:\n",
    "\n",
    "Each model sees a different up-sampled dataset\n",
    "\n",
    "6. Cost-Sensitive Learning (Alternative Approach)\n",
    "\n",
    "What it does:\n",
    "\n",
    "Penalizes misclassification of minority class more heavily\n",
    "\n",
    "Does not modify dataset size\n",
    "\n",
    "7. Keep Validation Data Realistic\n",
    "\n",
    "Best practice:\n",
    "\n",
    "Apply up-sampling only on training data\n",
    "\n",
    "Keep validation/test data unchanged\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501c0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83068df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
